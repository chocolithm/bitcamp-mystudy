
Exam : SAA-C03
Title : Amazon AWS Certified Solutions Architect - Associate (SAA-C03) Exam
Vendor : Amazon
Version : V14.95
IT Certification Guaranteed, The Easy Way!

NO.1
A company hosts its web application on AWS using seven Amazon EC2 instances. The company requires that the IP addresses of all healthy EC2 instances be returned in response to DNS queries. Which policy should be used to meet this requirement?
(A) Simple routing policy
(B) Latency routing policy
(C) Multivalue routing policy
(D) Geolocation routing policy
Answer: C

Explanation:
Use a multivalue answer routing policy to help distribute DNS responses across multiple resources. For example, use multivalue answer routing when you want to associate your routing records with a Route 53 health check. For example, use multivalue answer routing when you need to return multiple values for a DNS query and route traffic to multiple IP addresses.

NO.2
A company wants to reduce the cost of its existing three-tier web architecture. The web, application, and database servers are running on Amazon EC2 instances for the development, test, and production environments. The EC2 instances average 30% CPU utilization during peak hours and 10% CPU utilization during non-peak hours.
The production EC2 instances run 24 hours a day. The development and test EC2 instances run for at least 8 hours each day. The company plans to implement automation to stop the development and test EC2 instances when they are not in use.
Which EC2 instance purchasing solution will meet the company's requirements MOST cost-effectively?
(A) Use Spot Instances for the production EC2 instances. Use Reserved Instances for the development and test EC2 instances.
(B) Use Reserved Instances for the production EC2 instances. Use On-Demand Instances for the development and test EC2 instances.
(C) Use Spot blocks for the production EC2 instances. Use Reserved Instances for the development and test EC2 instances.
(D) Use On-Demand Instances for the production EC2 instances. Use Spot blocks for the development and test EC2 instances.
Answer: B

NO.3
A company hosts an application on multiple Amazon EC2 instances. The application processes messages from an Amazon SQS queue, writes to an Amazon RDS table, and deletes the message from the queue. Occasional duplicate records are found in the RDS table. The SQS queue does not contain any duplicate messages.
What should a solutions architect do to ensure messages are being processed once only?
(A) Use the CreateQueue API call to create a new queue
(B) Use the Add Permission API call to add appropriate permissions
(C) Use the ReceiveMessage API call to set an appropriate wait time
(D) Use the ChangeMessageVisibility API call to increase the visibility timeout
Answer: D

Explanation:
The visibility timeout begins when Amazon SQS returns a message. During this time, the consumer processes and deletes the message. However, if the consumer fails before deleting the message and your system doesn't call the DeleteMessage action for that message before the visibility timeout expires, the message becomes visible to other consumers and the message is received again.

NO.4
A company needs to migrate a legacy application from an on-premises data center to the AWS Cloud because of hardware capacity constraints. The application runs 24 hours a day, 7 days a week, and the application database storage continues to grow over time.
What should a solutions architect do to meet these requirements MOST cost-effectively?
(A) Migrate the application layer to Amazon EC2 Spot Instances. Migrate the data storage layer to Amazon S3.
(B) Migrate the application layer to Amazon EC2 Reserved Instances. Migrate the data storage layer to Amazon RDS On-Demand Instances.
(C) Migrate the application layer to Amazon EC2 Reserved Instances. Migrate the data storage layer to Amazon Aurora Reserved Instances.
(D) Migrate the application layer to Amazon EC2 On-Demand Instances. Migrate the data storage layer to Amazon RDS Reserved Instances.
Answer: C

NO.5
A company wants to run applications in containers in the AWS Cloud. These applications are stateless and can tolerate disruptions within the underlying infrastructure. The company needs a solution that minimizes cost and operational overhead.
What should a solutions architect do to meet these requirements?
(A) Use Spot Instances in an Amazon EC2 Auto Scaling group to run the application containers.
(B) Use Spot Instances in an Amazon Elastic Kubernetes Service (Amazon EKS) managed node group.
(C) Use On-Demand Instances in an Amazon EC2 Auto Scaling group to run the application containers.
(D) Use On-Demand Instances in an Amazon Elastic Kubernetes Service (Amazon EKS) managed node group.
Answer: A

NO.6
A business's backup data totals 700 terabytes (TB) and is kept in network-attached storage (NAS) at its data center. This backup data must be available in the event of occasional regulatory inquiries and preserved for a period of seven years. The organization has chosen to relocate its backup data from its on-premises data center to Amazon Web Services (AWS). Within one month, the migration must be completed. The company's public internet connection provides 500 Mbps of dedicated capacity for data transport.
What should a solutions architect do to ensure that data is migrated and stored at the LOWEST possible cost?
(A) Order AWS Snowball devices to transfer the data. Use a lifecycle policy to transition the files to Amazon S3 Glacier Deep Archive.
(B) Deploy a VPN connection between the data center and Amazon VPC. Use the AWS CLI to copy the data from on-premises to Amazon S3 Glacier.
(C) Provision a 500 Mbps AWS Direct Connect connection and transfer the data to Amazon S3. Use a lifecycle policy to transition the files to Amazon S3 Glacier Deep Archive.
(D) Use AWS DataSync to transfer the data and deploy a DataSync agent on-premises. Use the DataSync task to copy files from the on-premises NAS storage to Amazon S3 Glacier.
Answer: A

NO.7
An application development team is designing a microservice that will convert large images to smaller, compressed images. When a user uploads an image through the web interface, the microservice should store the image in an Amazon S3 bucket, process and compress the image with an AWS Lambda function, and store the image in its compressed form in a different S3 bucket.
A solutions architect needs to design a solution that uses durable, stateless components to process the images automatically.
Which combination of actions will meet these requirements? (Choose two.)
(A) Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure the S3 bucket to send a notification to the SQS queue when an image is uploaded to the S3 bucket.
(B) Configure the Lambda function to use the Amazon Simple Queue Service (Amazon SQS) queue as the invocation source. When the SQS message is successfully processed, delete the message in the queue.
(C) Configure the Lambda function to monitor the S3 bucket for new uploads. When an uploaded image is detected, write the file name to a text file in memory and use the text file to keep track of the images that were processed.
(D) Launch an Amazon EC2 instance to monitor an Amazon Simple Queue Service (Amazon SQS) queue. When items are added to the queue, log the file name in a text file on the EC2 instance and invoke the Lambda function.
(E) Configure an Amazon EventBridge (Amazon CloudWatch Events) event to monitor the S3 bucket. When an image is uploaded, send an alert to an Amazon Simple Notification Service (Amazon SNS) topic with the application owner's email address for further processing.
Answer: A, B

NO.8
A company has a large dataset for its online advertising business stored in an Amazon RDS for MySQL DB instance in a single Availability Zone. The company wants business reporting queries to run without impacting the write operations to the production DB instance.
Which solution meets these requirements?
(A) Deploy RDS read replicas to process the business reporting queries.
(B) Scale out the DB instance horizontally by placing it behind an Elastic Load Balancer.
(C) Scale up the DB instance to a larger instance type to handle write operations and queries.
(D) Deploy the DB instance in multiple Availability Zones to process the business reporting queries.
Answer: A

NO.9
A company is building an e-commerce web application on AWS. The application sends information about new orders to an Amazon API Gateway REST API to process. The company wants to ensure that orders are processed in the order that they are received.
Which solution will meet these requirements?
(A) Use an API Gateway integration to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic when the application receives an order. Subscribe an AWS Lambda function to the topic to perform processing.
(B) Use an API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS) FIFO queue when the application receives an order. Configure the SQS FIFO queue to invoke an AWS Lambda function for processing.
(C) Use an API Gateway authorizer to block any requests while the application processes an order.
(D) Use an API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS) standard queue when the application receives an order. Configure the SQS standard queue to invoke an AWS Lambda function for processing.
Answer: B

NO.10
A company needs to export its database once a day to Amazon S3 for other teams to access. The exported object size varies between 2 GB and 5 GB. The S3 access pattern for the data is variable and changes rapidly. The data must be immediately available and must remain accessible for up to 3 months. The company needs the most cost-effective solution that will not increase retrieval time. Which S3 storage class should the company use to meet these requirements?
(A) S3 Intelligent-Tiering
(B) S3 Standard-Infrequent Access (S3 Standard-IA)
(C) S3 One Zone-Infrequent Access (S3 One Zone-IA)
(D) S3 Glacier
Answer: B

Explanation:
S3 Standard-IA provides low-cost storage for infrequently accessed data but offers the same high durability, low latency, and high throughput performance as S3 Standard.

NO.11
A company has more than 5 TB of file data on Windows file servers that run on-premises. Users and applications interact with the data each day. The company is moving its Windows workloads to AWS. As the company continues this process, the company requires access to AWS and on-premises file storage with minimum latency. The company needs a solution that minimizes operational overhead and requires no significant changes to the existing file access patterns. The company uses an AWS Site-to-Site VPN connection for connectivity to AWS.
What should a solutions architect do to meet these requirements?
(A) Deploy and configure Amazon FSx for Windows File Server on AWS. Move the on-premises file data to FSx for Windows File Server. Reconfigure the workloads to use FSx for Windows File Server on AWS.
(B) Deploy and configure an Amazon S3 File Gateway on-premises. Move the on-premises file data to the S3 File Gateway. Reconfigure the on-premises workloads and the cloud workloads to use the S3 File Gateway.
(C) Deploy and configure an Amazon S3 File Gateway on-premises. Move the on-premises file data to Amazon S3. Reconfigure the workloads to use either Amazon S3 directly or the S3 File Gateway, depending on each workload's location.
(D) Deploy and configure Amazon FSx for Windows File Server on AWS. Deploy and configure an Amazon FSx File Gateway on-premises. Move the on-premises file data to the FSx File Gateway. Configure the cloud workloads to use FSx for Windows File Server on AWS. Configure the on-premises workloads to use the FSx File Gateway.
Answer: D

NO.12
A company recently announced the deployment of its retail website to a global audience. The website runs on multiple Amazon EC2 instances behind an Elastic Load Balancer. The instances run in an Auto Scaling group across multiple Availability Zones.
The company wants to provide its customers with different versions of content based on the devices that the customers use to access the website.
Which combination of actions should a solutions architect take to meet these requirements? (Choose two.)
(A) Configure Amazon CloudFront to cache multiple versions of the content.
(B) Configure a host header in a Network Load Balancer to forward traffic to different instances.
(C) Configure a Lambda@Edge function to send specific objects to users based on the User-Agent header.
(D) Configure AWS Global Accelerator. Forward requests to a Network Load Balancer (NLB). Configure the NLB to set up host-based routing to different EC2 instances.
(E) Configure AWS Global Accelerator. Forward requests to a Network Load Balancer (NLB). Configure the NLB to set up path-based routing to different EC2 instances.
Answer: A, C

Explanation:
For C: IMPROVED USER EXPERIENCE - Lambda@Edge can help improve your users' experience with your websites and web applications across the world by letting you personalize content for them without sacrificing performance. Real-time Image Transformation - You can customize your users' experience by transforming images on the fly based on the user characteristics. For example, you can resize images based on the viewer's device type-mobile, desktop, or tablet. You can also cache the transformed images at CloudFront Edge locations to further improve performance when delivering images.

NO.13
A company has a popular gaming platform running on AWS. The application is sensitive to latency because latency can impact the user experience and introduce unfair advantages to some players. The application is deployed in every AWS Region. It runs on Amazon EC2 instances that are part of Auto Scaling groups configured behind Application Load Balancers (ALBs). A solutions architect needs to implement a mechanism to monitor the health of the application and redirect traffic to healthy endpoints.
Which solution meets these requirements?
(A) Configure an accelerator in AWS Global Accelerator. Add a listener for the port that the application listens on, and attach it to a Regional endpoint in each Region. Add the ALB as the endpoint.
(B) Create an Amazon CloudFront distribution and specify the ALB as the origin server. Configure the cache behavior to use origin cache headers. Use AWS Lambda functions to optimize the traffic.
(C) Create an Amazon CloudFront distribution and specify Amazon S3 as the origin server. Configure the cache behavior to use origin cache headers. Use AWS Lambda functions to optimize the traffic.
(D) Configure an Amazon DynamoDB database to serve as the data store for the application. Create a DynamoDB Accelerator (DAX) cluster to act as the in-memory cache for DynamoDB hosting the application data.
Answer: A

Explanation:
AWS Global Accelerator directs traffic to the optimal healthy endpoint based on health checks, it can also route traffic to the closest healthy endpoint based on the geographic location of the client. By configuring an accelerator and attaching it to a Regional endpoint in each Region, and adding the ALB as the endpoint, the solution will redirect traffic to healthy endpoints, improving the user experience by reducing latency and ensuring that the application is running optimally.

NO.14
A company has launched an Amazon RDS for MySQL DB instance. Most of the connections to the database come from serverless applications. Application traffic to the database changes significantly at random intervals. At times of high demand, users report that their applications experience database connection rejection errors.
Which solution will resolve this issue with the LEAST operational overhead?
(A) Create a proxy in RDS Proxy. Configure the users' applications to use the DB instance through RDS Proxy.
(B) Deploy Amazon ElastiCache for Memcached between the users' application and the DB instance.
(C) Migrate the DB instance to a different instance class that has higher I/O capacity. Configure the users' applications to use the new DB instance.
(D) Configure Multi-AZ for the DB instance. Configure the users' application to switch between the DB instances.
Answer: A

NO.15
A company has a web application hosted over 10 Amazon EC2 instances with traffic directed by Amazon Route 53. The company occasionally experiences a timeout error when attempting to browse the application.
The networking team finds that some DNS queries return IP addresses of unhealthy instances, resulting in the timeout error.
What should a solutions architect implement to overcome these timeout errors?
(A) Create a Route 53 simple routing policy record for each EC2 instance. Associate a health check with each record.
(B) Create a Route 53 failover routing policy record for each EC2 instance. Associate a health check with each record.
(C) Create an Amazon CloudFront distribution with EC2 instances as its origin. Associate a health check with the EC2 instances.
(D) Create an Application Load Balancer (ALB) with a health check in front of the EC2 instances. Route to the ALB from Route 53.
Answer: D

Explanation:
An Application Load Balancer (ALB) allows you to distribute incoming traffic across multiple backend instances, and can automatically route traffic to healthy instances while removing traffic from unhealthy instances. By using an ALB in front of the EC2 instances and routing traffic to it from Route 53, the load balancer can perform health checks on the instances and only route traffic to healthy instances, which should help to reduce or eliminate timeout errors caused by unhealthy instances.

NO.16
A company wants to migrate its 1 PB on-premises image repository to AWS. The images will be used by a serverless web application. Images stored in the repository are rarely accessed, but they must be immediately available. Additionally, the images must be encrypted at rest and protected from accidental deletion. Which solution meets these requirements?
(A) Implement client-side encryption and store the images in an Amazon S3 Glacier vault. Set a vault lock to prevent accidental deletion.
(B) Store the images in an Amazon S3 bucket in the S3 Standard-Infrequent Access (S3 Standard-IA) storage class. Enable versioning, default encryption, and MFA Delete on the S3 bucket.
(C) Store the images in an Amazon FSx for Windows File Server file share. Configure the Amazon FSx file share to use an AWS Key Management Service (AWS KMS) customer master key (CMK) to encrypt the images in the file share. Use NTFS permission sets on the images to prevent accidental deletion.
(D) Store the images in an Amazon Elastic File System (Amazon EFS) file share in the Infrequent Access storage class. Configure the EFS file share to use an AWS Key Management Service (AWS KMS) customer master key (CMK) to encrypt the images in the file share. Use NFS permission sets on the images to prevent accidental deletion.
Answer: B

NO.17
A company is designing an application where users upload small files into Amazon S3. After a user uploads a file, the file requires one-time simple processing to transform the data and save the data in JSON format for later analysis.
Each file must be processed as quickly as possible after it is uploaded. Demand will vary. On some days, users will upload a high number of files. On other days, users will upload a few files or no files.
Which solution meets these requirements with the LEAST operational overhead?
(A) Configure Amazon EMR to read text files from Amazon S3. Run processing scripts to transform the data. Store the resulting JSON file in an Amazon Aurora DB cluster.
(B) Configure Amazon S3 to send an event notification to an Amazon Simple Queue Service (Amazon SQS) queue. Use Amazon EC2 instances to read from the queue and process the data. Store the resulting JSON file in Amazon DynamoDB.
(C) Configure Amazon S3 to send an event notification to an Amazon Simple Queue Service (Amazon SQS) queue. Use an AWS Lambda function to read from the queue and process the data. Store the resulting JSON file in Amazon DynamoDB.
(D) Configure Amazon EventBridge (Amazon CloudWatch Events) to send an event to Amazon Kinesis Data Streams when a new file is uploaded. Use an AWS Lambda function to consume the event from the stream and process the data. Store the resulting JSON file in Amazon Aurora DB cluster.
Answer: C

NO.18
A company recently created a disaster recovery site in a different AWS Region. The company needs to transfer large amounts of data back and forth between NFS file systems in the two Regions on a periodic basis.
Which solution will meet these requirements with the LEAST operational overhead?
(A) Use AWS DataSync.
(B) Use AWS Snowball devices.
(C) Set up an SFTP server on Amazon EC2.
(D) Use AWS Database Migration Service (AWS DMS).
Answer: A

NO.19
A company has a Windows-based application that must be migrated to AWS. The application requires the use of a shared Windows file system attached to multiple Amazon EC2 Windows instances that are deployed across multiple Availability Zones.
What should a solutions architect do to meet this requirement?
(A) Configure AWS Storage Gateway in volume gateway mode. Mount the volume to each Windows instance.
(B) Configure Amazon FSx for Windows File Server. Mount the Amazon FSx file system to each Windows instance.
(C) Configure a file system by using Amazon Elastic File System (Amazon EFS). Mount the EFS file system to each Windows instance.
(D) Configure an Amazon Elastic Block Store (Amazon EBS) volume with the required size. Attach each EC2 instance to the volume. Mount the file system within the volume to each Windows instance.
Answer: B

NO.20
A company has a three-tier application on AWS that ingests sensor data from its users' devices. The traffic flows through a Network Load Balancer (NLB), then to Amazon EC2 instances for the web tier, and finally to EC2 instances for the application tier. The application tier makes calls to a database.
What should a solutions architect do to improve the security of the data in transit?
(A) Create a Network Load Balancer (NLB) and use a TLS listener to terminate the connection, then use the Load Balancer to send traffic to the backend targets over TCP.
(B) Configure a TLS listener on the existing Network Load Balancer (NLB). Use Server Name Indication (SNI) to present different certificates based on the client request.
(C) Use a Virtual Private Cloud (VPC) with a PrivateLink to establish a secure connection between the application and the database.
(D) Add an Application Load Balancer (ALB) with a TLS listener between the Network Load Balancer and the web tier to decrypt traffic.
Answer: B

Explanation:
Configuring a TLS listener on the Network Load Balancer (NLB) with SNI provides a secure solution by decrypting traffic before sending it to the backend targets.

NO.21
An application that is hosted on Amazon EC2 instances needs to access an Amazon S3 bucket. Traffic must not traverse the internet. How should a solutions architect configure access to meet these requirements?
(A) Create a private hosted zone by using Amazon Route 53.
(B) Set up a gateway VPC endpoint for Amazon S3 in the VPC.
(C) Configure the EC2 instances to use a NAT gateway to access the S3 bucket.
(D) Establish an AWS Site-to-Site VPN connection between the VPC and the S3 bucket.
Answer: B

Explanation:
Using a VPC endpoint ensures that traffic between the EC2 instances and S3 does not leave the AWS network, providing secure access without traversing the internet.

NO.22
A company is hosting a three-tier e-commerce application in the AWS Cloud. The company hosts the website on Amazon S3 and integrates the website with an API that handles sales requests. The company hosts the API on three Amazon EC2 instances behind an Application Load Balancer (ALB). The API consists of static and dynamic front-end content along with backend workers that process sales requests asynchronously.
The company is expecting a significant and sudden increase in the number of sales requests during events for the launch of new products. What should a solutions architect recommend to ensure that all the requests are processed successfully?
(A) Add an Amazon CloudFront distribution for the dynamic content. Increase the number of EC2 instances to handle the increase in traffic.
(B) Add an Amazon CloudFront distribution for the static content. Place the EC2 instances in an Auto Scaling group to launch new instances based on network traffic.
(C) Add an Amazon CloudFront distribution for the dynamic content. Add an Amazon ElastiCache instance in front of the ALB to reduce traffic for the API to handle.
(D) Add an Amazon CloudFront distribution for the static content. Add an Amazon Simple Queue Service (Amazon SQS) queue to receive requests from the website for later processing by the EC2 instances.
Answer: D

NO.23
What should a solutions architect do to ensure that all objects uploaded to an Amazon S3 bucket are encrypted?
(A) Update the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set.
(B) Update the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set to private.
(C) Update the bucket policy to deny if the PutObject does not have an aws:SecureTransport header set to true.
(D) Update the bucket policy to deny if the PutObject does not have an x-amz-server-side-encryption header set.
Answer: D

Explanation:
Updating the bucket policy to deny if the PutObject request does not have an x-amz-server-side-encryption header set ensures that all objects uploaded to the S3 bucket are encrypted.

NO.24
A company provides an API to its users that automates inquiries for tax computations based on item prices.
The company experiences a larger number of inquiries during the holiday season only that cause slower response times. A solutions architect needs to design a solution that is scalable and elastic.
What should the solutions architect do to accomplish this?
(A) Provide an API hosted on an Amazon EC2 instance. The EC2 instance performs the required computations when the API request is made.
(B) Design a REST API using Amazon API Gateway that accepts the item names. API Gateway passes item names to AWS Lambda for tax computations.
(C) Create an Application Load Balancer that has two Amazon EC2 instances behind it. The EC2 instances will compute the tax on the received item names.
(D) Design a REST API using Amazon API Gateway that connects with an API hosted on an Amazon EC2 instance. API Gateway accepts and passes the item names to the EC2 instance for tax computations.
Answer: B

Explanation:
Using Amazon API Gateway with AWS Lambda is a scalable and elastic solution that can handle variable demand, such as the increased number of inquiries during the holiday season.

NO.25
As part of budget planning, management wants a report of AWS billed amounts listed by user. The data will be used to create department budgets. A solutions architect needs to determine the most efficient way to obtain this report information.
Which solution meets these requirements?
(A) Access the bill details from the running dashboard and download via bill.
(B) Create a report in Cost Explorer and download the report.
(C) Run a query with Amazon Athena to generate the report.
(D) Modify a cost budget in AWS Budgets to alert with Amazon Simple Email Service (Amazon SES).
Answer: B

Explanation:
Creating a report in Cost Explorer is the most efficient way to obtain a detailed report of AWS billed amounts by user. It provides the necessary data and can be downloaded for further analysis.

NO.26
A company stores call transcript files on a monthly basis. Users access the files randomly within 1 year of the call, but users access the files infrequently after 1 year. The company wants to optimize its solution by giving users the ability to query and retrieve files that are less than 1-year-old as quickly as possible. A delay in retrieving older files is acceptable.
Which solution will meet these requirements MOST cost-effectively?
(A) Store individual files with tags in Amazon S3 Glacier Instant Retrieval. Query the tags to retrieve the files from S3 Glacier Instant Retrieval.
(B) Store individual files in Amazon S3 Intelligent-Tiering. Use S3 Lifecycle policies to move the files to S3 Glacier Flexible Retrieval after 1 year. Query and retrieve the files that are in Amazon S3 by using Amazon Athena. Query and retrieve the files that are in S3 Glacier by using S3 Glacier Select.
(C) Store individual files with tags in Amazon S3 Standard storage. Store search metadata for each archive in Amazon S3 Standard storage. Use S3 Lifecycle policies to move the files to S3 Glacier Instant Retrieval after 1 year. Query and retrieve the files by searching for metadata from Amazon S3.
(D) Store individual files in Amazon S3 Standard storage. Use S3 Lifecycle policies to move the files to S3 Glacier Deep Archive after 1 year. Store search metadata in Amazon RDS. Query the files from Amazon RDS. Retrieve the files from S3 Glacier Deep Archive.
Answer: B

NO.27
A company sells ringtones created from clips of popular songs. The files containing the ringtones are stored in Amazon S3 Standard and are at least 128 KB in size. The company has millions of files, but downloads are infrequent for ringtones older than 90 days. The company needs to save money on storage while keeping the most accessed files readily available for its users.
Which action should the company take to meet these requirements MOST cost-effectively?
(A) Configure S3 Standard-Infrequent Access (S3 Standard-IA) storage for the initial storage tier of the objects.
(B) Move the files to S3 Intelligent-Tiering and configure it to move objects to a less expensive storage tier after 90 days.
(C) Configure S3 inventory to manage objects and move them to S3 Standard-Infrequent Access (S3 Standard-IA) after 90 days.
(D) Implement an S3 Lifecycle policy that moves the objects from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) after 90 days.
Answer: D

NO.28
An e-commerce company is running a multi-tier application on AWS. The front-end and backend tiers run on Amazon EC2, and the database runs on Amazon RDS for MySQL. The backend tier communicates with the RDS instance. There are frequent calls to return identical data from the database that are causing performance slowdowns.
Which action should be taken to improve the performance of the backend?
(A) Implement Amazon SNS to store the database calls.
(B) Implement Amazon ElastiCache to cache the large database.
(C) Implement an RDS for MySQL read replica to cache database calls.
(D) Implement Amazon Kinesis Data Firehose to stream the calls to the database.
Answer: B

Explanation:
Using Amazon ElastiCache to cache frequently accessed data can significantly improve the performance of the backend by reducing the load on the database and minimizing latency for read-heavy workloads.

NO.29
A solutions architect is designing a company's disaster recovery (DR) architecture. The company has a MySQL database that runs on an Amazon EC2 instance in a private subnet with scheduled backup. The DR design needs to include multiple AWS Regions.
Which solution will meet these requirements with the LEAST operational overhead?
(A) Migrate the MySQL database to multiple EC2 instances. Configure a standby EC2 instance in the DR Region. Turn on replication.
(B) Migrate the MySQL database to Amazon RDS. Use a Multi-AZ deployment. Turn on read replication for the primary DB instance in the different Availability Zones.
(C) Migrate the MySQL database to an Amazon Aurora global database. Host the primary DB cluster in the primary Region. Host the secondary DB cluster in the DR Region.
(D) Store the scheduled backup of the MySQL database in an Amazon S3 bucket that is configured for S3 Cross-Region Replication (CRR). Use the data backup to restore the database in the DR Region.
Answer: C

NO.30
A company wants to use the AWS Cloud to make an existing application highly available and resilient. The current version of the application resides in the company's data center. The application recently experienced data loss after a database server crashed because of an unexpected power outage. The company needs a solution that avoids any single points of failure. The solution must give the application the ability to scale to meet user demand.
Which solution will meet these requirements?
(A) Migrate the application to Amazon EC2 instances that run in multiple Availability Zones. Use Amazon RDS with Multi-AZ to host the database.
(B) Migrate the application to Amazon EC2 instances behind an Application Load Balancer in a single Availability Zone. Use Amazon DynamoDB to host the database.
(C) Migrate the application to Amazon EC2 instances behind an Application Load Balancer in multiple Availability Zones. Use Amazon DynamoDB to host the database.
(D) Migrate the application to Amazon EC2 instances that run in a single Availability Zone. Use Amazon RDS with Multi-AZ to host the database.
Answer: A

Explanation:
Using Amazon EC2 instances in multiple Availability Zones with an Amazon RDS Multi-AZ deployment ensures high availability and resilience by eliminating single points of failure and providing scalability to meet user demand.

NO.31
A company is deploying a new application on Amazon EC2 instances. The application writes data to Amazon Elastic Block Store (Amazon EBS) volumes. The company needs to ensure that all data that is written to the EBS volumes is encrypted at rest.
Which solution will meet this requirement?
(A) Create an IAM role that specifies EBS encryption. Attach the role to the EC2 instances.
(B) Create the EBS volumes as encrypted volumes. Attach the EBS volumes to the EC2 instances.
(C) Create an EC2 instance tag that has a key of Encrypt and a value of True. Tag all instances that require encryption at the EBS level.
(D) Create an AWS Key Management Service (AWS KMS) key policy that enforces EBS encryption in the account. Ensure that the key policy is active.
Answer: B

Explanation:
Creating the EBS volumes as encrypted volumes ensures that all data written to them is encrypted at rest, meeting the security requirement.

NO.32
A company must migrate 20 TB of data from a data center to the AWS Cloud within 30 days. The company's network bandwidth is limited to 15 Mbps and cannot exceed 70% utilization.
What should a solutions architect do to meet these requirements?
(A) Use AWS Snowball.
(B) Use AWS DataSync.
(C) Use a secure VPN connection.
(D) Use Amazon S3 Transfer Acceleration.
Answer: A

Explanation:
AWS Snowball is a secure and efficient data transfer service designed for large amounts of data. It is well-suited for migrating 20 TB of data within a limited bandwidth environment, meeting the 30-day deadline.

NO.33
A company is developing a new mobile app. The company must implement proper traffic filtering to protect its Application Load Balancer (ALB) against common application-level attacks, such as cross-site scripting or SQL injection. The company has minimal infrastructure and operational staff. The company needs to reduce its share of the responsibility in managing, updating, and securing servers for its AWS environment.
What should a solutions architect recommend to meet these requirements?
(A) Configure AWS WAF rules and associate them with the ALB.
(B) Deploy the application using Amazon S3 with public hosting enabled.
(C) Deploy AWS Shield Advanced and add the ALB as a protected resource.
(D) Create a new ALB that directs traffic to an Amazon EC2 instance running a third-party firewall, which then passes the traffic to the current ALB.
Answer: A

Explanation:
AWS WAF (Web Application Firewall) provides protection against common application-level attacks like cross-site scripting and SQL injection. Associating WAF rules with the ALB reduces the company’s responsibility for managing security, making it an effective solution for the given requirements.

NO.34
A company hosts a two-tier application on Amazon EC2 instances and Amazon RDS. The application’s demand varies based on the time of day. The load is minimal after work hours and on weekends. The EC2 instances run in an EC2 Auto Scaling group that is configured with a minimum of two instances and a maximum of five instances. The application must be available at all times, but the company is concerned about overall cost.
Which solution meets the availability requirement MOST cost-effectively?
(A) Use all EC2 Spot Instances. Stop the RDS database when it is not in use.
(B) Purchase EC2 Instance Savings Plans to cover five EC2 instances. Purchase an RDS Reserved DB Instance.
(C) Purchase two EC2 Reserved Instances. Use up to three additional EC2 Spot Instances as needed. Stop the RDS database when it is not in use.
(D) Purchase EC2 Instance Savings Plans to cover two EC2 instances. Use up to three additional EC2 On-Demand Instances as needed. Purchase an RDS Reserved DB Instance.
Answer: D

Explanation:
Purchasing EC2 Instance Savings Plans for the base load of two instances and using On-Demand Instances for scaling needs, combined with an RDS Reserved DB Instance, balances cost and availability effectively.

NO.35
A solutions architect must create a disaster recovery (DR) plan for a high-volume software as a service (SaaS) platform. All data for the platform is stored in an Amazon Aurora MySQL DB cluster. The DR plan must replicate data to a secondary AWS Region.
Which solution will meet these requirements MOST cost-effectively?
(A) Use MySQL binary log replication to an Aurora cluster in the secondary Region. Provision one DB instance for the Aurora cluster in the secondary Region.
(B) Set up an Aurora global database for the DB cluster. When setup is complete, remove the DB instance from the secondary Region.
(C) Use AWS Database Migration Service (AWS DMS) to continuously replicate data to an Aurora cluster in the secondary Region. Remove the DB instance from the secondary Region.
(D) Set up an Aurora global database for the DB cluster. Specify a minimum of one DB instance in the secondary Region.
Answer: D

Explanation:
Setting up an Aurora global database with a minimum of one DB instance in the secondary Region provides a cost-effective solution for data replication in a disaster recovery scenario.

NO.36
A company needs to transfer 600 TB of data from its on-premises network-attached storage (NAS) system to the AWS Cloud. The data transfer must be complete within 2 weeks. The data is sensitive and must be encrypted in transit. The company's internet connection can support an upload speed of 100 Mbps.
Which solution meets these requirements MOST cost-effectively?
(A) Use Amazon S3 multi-part upload functionality to transfer the files over HTTPS.
(B) Create a VPN connection between the on-premises NAS system and the nearest AWS Region. Transfer the data over the VPN connection.
(C) Use the AWS Snow Family console to order several AWS Snowball Edge Storage Optimized devices. Use the devices to transfer the data to Amazon S3.
(D) Set up a 10 Gbps AWS Direct Connect connection between the company location and the nearest AWS Region. Transfer the data over a VPN connection into the Region to store the data in Amazon S3.
Answer: C

Explanation:
Using AWS Snowball Edge devices is the most cost-effective and efficient solution for transferring large amounts of data (600 TB) securely within the 2-week timeframe.

NO.37
A solutions architect needs to help a company optimize the cost of running an application on AWS. The application will use Amazon EC2 instances, AWS Fargate, and AWS Lambda for compute within the architecture.
The EC2 instances will run the data ingestion layer of the application. EC2 usage will be sporadic and unpredictable. Workloads that run on EC2 instances can be interrupted at any time. The application front end will run on Fargate, and Lambda will serve the API layer. The front-end utilization and API layer utilization will be predictable over the course of the next year.
Which combination of purchasing options will provide the MOST cost-effective solution for hosting this application? (Choose two.)
(A) Use Spot Instances for the data ingestion layer.
(B) Use On-Demand Instances for the data ingestion layer.
(C) Purchase a 1-year Compute Savings Plan for the front end and API layer.
(D) Purchase 1-year All Upfront Reserved Instances for the data ingestion layer.
(E) Purchase a 1-year EC2 instance Savings Plan for the front end and API layer.
Answer: A, C

Explanation:
Using Spot Instances for the data ingestion layer leverages cost savings due to the sporadic and interruptible nature of the workload. A Compute Savings Plan for the predictable front-end and API layer ensures cost optimization.

NO.38
A company needs the ability to analyze the log files of its proprietary application. The logs are stored in JSON format in an Amazon S3 bucket. Queries will be simple and will run on-demand. A solutions architect needs to perform the analysis with minimal changes to the existing architecture.
What should the solutions architect do to meet these requirements with the LEAST amount of operational overhead?
(A) Use Amazon Redshift to load all the content into one place and run the SQL queries as needed.
(B) Use Amazon CloudWatch Logs to store the logs. Run SQL queries as needed from the Amazon CloudWatch console.
(C) Use Amazon Athena directly with Amazon S3 to run the queries as needed.
(D) Use AWS Glue to catalog the logs. Use a transient Apache Spark cluster on Amazon EMR to run the SQL queries as needed.
Answer: C

Explanation:
Amazon Athena allows direct querying of data stored in Amazon S3 with minimal setup, making it the ideal solution for running on-demand queries on log files in JSON format with low operational overhead.

NO.39
A solutions architect needs to securely store a database user name and password that an application uses to access an Amazon RDS DB instance. The application that accesses the database runs on an Amazon EC2 instance. The solutions architect wants to create a secure parameter in AWS Systems Manager Parameter Store.
What should the solutions architect do to meet this requirement?
(A) Create an IAM role that has read access to the Parameter Store parameter. Allow Decrypt access to an AWS Key Management Service (AWS KMS) key that is used to encrypt the parameter. Assign this IAM role to the EC2 instance.
(B) Create an IAM policy that allows read access to the Parameter Store parameter. Allow Decrypt access to an AWS Key Management Service (AWS KMS) key that is used to encrypt the parameter. Assign this IAM policy to the EC2 instance.
(C) Create an IAM trust relationship between the Parameter Store parameter and the EC2 instance. Specify Amazon RDS as a principal in the trust policy.
(D) Create an IAM trust relationship between the DB instance and the EC2 instance. Specify Systems Manager as a principal in the trust policy.
Answer: B

Explanation:
Using an IAM policy to allow read access to the secure parameter in Parameter Store and Decrypt access to the associated KMS key is the correct approach. Assigning the policy to the EC2 instance ensures secure access to the credentials.

NO.40
A solutions architect observes that a nightly batch processing job is automatically scaled up for 1 hour before the desired Amazon EC2 capacity is reached. The peak capacity is the same every night, and the batch jobs always start at 1 AM. The solutions architect needs to find a cost-effective solution that will allow for the desired EC2 capacity to be reached quickly and allow the Auto Scaling group to scale down after the batch jobs are complete.
What should the solutions architect do to meet these requirements?
(A) Configure an Amazon CloudWatch alarm to send a notification when the EC2 instances are running at full capacity.
(B) Configure a predictive scaling policy in the Auto Scaling group to scale out based on forecasted capacity.
(C) Configure scheduled scaling to scale out EC2 instances before 1 AM.
(D) Configure dynamic scaling to scale out EC2 instances when CPU utilization exceeds a threshold.
Answer: C

Explanation:
Scheduled scaling is the most cost-effective solution because it ensures that the EC2 instances are scaled out before the batch jobs start at 1 AM, allowing for the desired capacity to be reached quickly. After the jobs are complete, the scaling can be reduced as needed.

NO.41
A company is running a critical business application on Amazon EC2 instances behind an Application Load Balancer. The EC2 instances run in an Auto Scaling group and access an Amazon RDS DB instance. The design did not pass an operational review because the EC2 instances and the DB instance are all located in a single Availability Zone. A solutions architect must update the design to use a second Availability Zone.
Which solution will make the application highly available?
(A) Provision a subnet in each Availability Zone. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance with connections to each network.
(B) Provision two subnets that extend across both Availability Zones. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance with connections to each network.
(C) Provision a subnet in each Availability Zone. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance for Multi-AZ deployment.
(D) Provision a subnet that extends across both Availability Zones. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance for Multi-AZ deployment.
Answer: C

Explanation:
By configuring the Auto Scaling group to distribute EC2 instances across both Availability Zones and setting up the DB instance for Multi-AZ deployment, the solution ensures high availability and fault tolerance.

NO.42
A company is migrating a distributed application to AWS. The application serves variable workloads. The legacy platform consists of a primary server that coordinates jobs across multiple compute nodes. The company wants to modernize the application with a solution that maximizes resiliency and scalability.
How should a solutions architect design the architecture to meet these requirements?
(A) Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination for the jobs. Implement the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. Configure EC2 Auto Scaling to use scheduled scaling.
(B) Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination for the jobs. Implement the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. Configure EC2 Auto Scaling based on the size of the queue.
(C) Implement the primary server and the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. Configure AWS CloudTrail as a destination for the jobs. Configure EC2 Auto Scaling based on the load on the primary server.
(D) Implement the primary server and the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. Configure Amazon EventBridge (Amazon CloudWatch Events) as a destination for the jobs. Configure EC2 Auto Scaling based on the load on the compute nodes.
Answer: B

Explanation:
Using Amazon SQS to decouple the job distribution and implementing Auto Scaling based on the size of the queue ensures that the system can scale elastically and handle variable workloads with high resiliency.

NO.43
A company has an application that runs on several Amazon EC2 instances. Each EC2 instance has multiple Amazon Elastic Block Store (Amazon EBS) data volumes attached to it. The application’s EC2 instance configuration and data need to be backed up nightly. The application also needs to be recoverable in a different AWS Region.
Which solution will meet these requirements in the MOST operationally efficient way?
(A) Write an AWS Lambda function that schedules nightly snapshots of the application’s EBS volumes and copies the snapshots to a different Region.
(B) Create a backup plan by using AWS Backup to perform nightly backups. Copy the backups to another Region. Add the application’s EC2 instances as resources.
(C) Create a backup plan by using AWS Backup to perform nightly backups. Copy the backups to another Region. Add the application’s EBS volumes as resources.
(D) Write an AWS Lambda function that schedules nightly snapshots of the application’s EBS volumes and copies the snapshots to a different Availability Zone.
Answer: C

Explanation:
Using AWS Backup with a backup plan that includes the EBS volumes as resources ensures automated nightly backups with minimal operational overhead. Copying the backups to another Region enhances disaster recovery capabilities.

NO.44
A company needs a backup strategy for its three-tier stateless web application. The web application runs on Amazon EC2 instances in an Auto Scaling group with a dynamic scaling policy that is configured to respond to scaling events. The database tier runs on Amazon RDS for PostgreSQL. The web application does not require temporary local storage on the EC2 instances. The company’s recovery point objective (RPO) is 2 hours. The backup strategy must maximize scalability and optimize resource utilization for this environment.
Which solution will meet these requirements?
(A) Take snapshots of Amazon Elastic Block Store (Amazon EBS) volumes of the EC2 instances and database every 2 hours to meet the RPO.
(B) Configure a snapshot lifecycle policy to take Amazon Elastic Block Store (Amazon EBS) snapshots. Enable automated backups in Amazon RDS to meet the RPO.
(C) Retain the latest Amazon Machine Images (AMIs) of the web and application tiers. Enable automated backups in Amazon RDS and use point-in-time recovery to meet the RPO.
(D) Take snapshots of Amazon Elastic Block Store (Amazon EBS) volumes of the EC2 instances every 2 hours. Enable automated backups in Amazon RDS and use point-in-time recovery to meet the RPO.
Answer: D

Explanation:
Taking regular snapshots of the EBS volumes and enabling automated backups in RDS, combined with point-in-time recovery, meets the 2-hour RPO requirement while maximizing scalability and optimizing resource utilization.

NO.45
A company uses a legacy application to produce data in CSV format. The legacy application stores the output data in Amazon S3. The company is deploying a new commercial off-the-shelf (COTS) application that can perform complex SQL queries to analyze data that is stored in Amazon Redshift and Amazon S3 only. However, the COTS application cannot process the CSV files that the legacy application produces. The company cannot update the legacy application to produce data in another format. The company needs to implement a solution so that the COTS application can use the data that the legacy application produces.
Which solution will meet these requirements with the LEAST operational overhead?
(A) Create an AWS Glue extract, transform, and load (ETL) job that runs on a schedule. Configure the ETL job to process the CSV files and store the processed data in Amazon Redshift.
(B) Develop a Python script that runs on Amazon EC2 instances to convert the CSV files to SQL files. Invoke the Python script on a cron schedule to store the output files in Amazon S3.
(C) Create an AWS Lambda function and an Amazon DynamoDB table. Use an S3 event to invoke the Lambda function. Configure the Lambda function to perform an extract, transform, and load (ETL) job to process the CSV files and store the processed data in the DynamoDB table

